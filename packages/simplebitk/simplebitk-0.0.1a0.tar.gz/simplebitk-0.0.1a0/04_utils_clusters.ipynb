{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils.clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.cluster import MiniBatchKMeans,AgglomerativeClustering,\\\n",
    "                            SpectralClustering,DBSCAN,OPTICS,AffinityPropagation,\\\n",
    "                            AgglomerativeClustering,Birch\n",
    "from sklearn.metrics import silhouette_score,calinski_harabasz_score,davies_bouldin_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def clusters_annotation(df,method,params):\n",
    "    if method not in [MiniBatchKMeans,AgglomerativeClustering,\n",
    "                      SpectralClustering,DBSCAN,OPTICS,AffinityPropagation,\n",
    "                      AgglomerativeClustering,Birch]:\n",
    "        raise ValueError('method should be in sklearn.cluster.*, e.g. DBSCAN')\n",
    "    if method in [MiniBatchKMeans,AgglomerativeClustering,SpectralClustering,Birch]:\n",
    "        cluster= method(n_clusters=params['n_clusters'])\n",
    "    elif method in [DBSCAN,OPTICS]:\n",
    "        cluster=method(eps=params['eps'])\n",
    "    elif method == AffinityPropagation:\n",
    "        cluster=method(damping=params['damping'], preference=params['preference'])\n",
    "    clustering = cluster.fit_predict(df)\n",
    "    return clustering\n",
    "\n",
    "ass_methods={\n",
    "    'silhouette_score':silhouette_score,\n",
    "    'calinski_harabasz_score':calinski_harabasz_score,\n",
    "    'davies_bouldin_score':davies_bouldin_score\n",
    "}\n",
    "\n",
    "cluster_methods={\n",
    "    'MiniBatchKMeans':MiniBatchKMeans,\n",
    "    'AgglomerativeClustering':AgglomerativeClustering,\n",
    "    'SpectralClustering':SpectralClustering,\n",
    "    'DBSCAN':DBSCAN,\n",
    "    'OPTICS':OPTICS,\n",
    "    'AffinityPropagation':AffinityPropagation,\n",
    "    'AgglomerativeClustering':AgglomerativeClustering,\n",
    "    'Birch':Birch\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from simplebitk.utils.plots import scatter_plots_for_reduce_dimensional\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "df = pd.DataFrame(X,columns=['x1','x2','x3','x4'])\n",
    "df['dbscan']=clusters_annotation(df,cluster_methods['DBSCAN'],{'eps':0.3})\n",
    "scatter_plots_for_reduce_dimensional(df,'x1',\n",
    "                                     'x2',hue='dbscan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def find_peak_valley(sequence,peak=True):\n",
    "    if peak:\n",
    "        peaks, _ =  find_peaks(sequence)\n",
    "        return peaks\n",
    "    else:\n",
    "        peaks, _ = find_peaks(-sequence)\n",
    "        return peaks\n",
    "    \n",
    "    \n",
    "def find_best_cluster_number(df,cluster_method,params,ass_method=silhouette_score):\n",
    "    records = []\n",
    "    if cluster_method in [MiniBatchKMeans,AgglomerativeClustering,SpectralClustering,Birch]:\n",
    "        for i in range(2,20):\n",
    "            params['n_clusters'] = i\n",
    "            clustering = clusters_annotation(df,cluster_method,params)\n",
    "            records.append([i,ass_method(df,clustering)])\n",
    "    elif cluster_method in [DBSCAN,OPTICS]:\n",
    "        for i in np.arange(0.1,4,0.2):\n",
    "            params['eps']=i\n",
    "            clustering = clusters_annotation(df,cluster_method,params)\n",
    "            if sum(clustering) == -len(clustering):\n",
    "                records.append([i,0])\n",
    "            else:\n",
    "                records.append([i,ass_method(df,clustering)])\n",
    "        \n",
    "    records = np.array(records)\n",
    "#     peaks, _ =  find_peaks(records[:,1])\n",
    "    if ass_method == silhouette_score:\n",
    "        peaks = find_peak_valley(records[:,1])\n",
    "        if len(peaks) == 0:\n",
    "            return records,records,peaks\n",
    "        return records[peaks[0]],records,peaks\n",
    "    elif ass_method == calinski_harabasz_score:\n",
    "        peaks = find_peak_valley(records[:,1])\n",
    "        if len(peaks) == 0:\n",
    "            return records,records,peaks\n",
    "        return records[peaks[0]],records,peaks\n",
    "    elif ass_method == davies_bouldin_score:\n",
    "        peaks = find_peak_valley(records[:,1],False)\n",
    "        if len(peaks) == 0:\n",
    "            return records,records,peaks\n",
    "        return records[peaks[0]],records,peaks\n",
    "    else:\n",
    "        raise ValueError('ass method can only be one of [silhouette_score,calinski_harabasz_score,davies_bouldin_score]')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best cluster number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.9        -0.01688904]\n",
      "[14 18]\n"
     ]
    }
   ],
   "source": [
    "X=np.random.normal(3,4,(100,4))\n",
    "i=silhouette_score\n",
    "a,records,peaks = find_best_cluster_number(X,DBSCAN,{'n_clusters':3,'eps':0.3},ass_method=i)\n",
    "\n",
    "plt.plot(records[:,0],records[:,1])\n",
    "plt.plot(records[peaks,0], records[peaks,1], \"x\")\n",
    "\n",
    "print(a)\n",
    "print(peaks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.9        5.05533082]\n",
      "[14 18]\n"
     ]
    }
   ],
   "source": [
    "i=calinski_harabasz_score\n",
    "a,records,peaks = find_best_cluster_number(X,DBSCAN,{'n_clusters':3,'eps':0.3},ass_method=i)\n",
    "\n",
    "plt.plot(records[:,0],records[:,1])\n",
    "plt.plot(records[peaks,0], records[peaks,1], \"x\")\n",
    "\n",
    "print(a)\n",
    "print(peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.5       3.2772862]\n",
      "[17]\n"
     ]
    }
   ],
   "source": [
    "i=davies_bouldin_score\n",
    "a,records,peaks = find_best_cluster_number(X,DBSCAN,{'n_clusters':3,'eps':0.3},ass_method=i)\n",
    "\n",
    "plt.plot(records[:,0],records[:,1])\n",
    "plt.plot(records[peaks,0], records[peaks,1], \"x\")\n",
    "\n",
    "print(a)\n",
    "print(peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
