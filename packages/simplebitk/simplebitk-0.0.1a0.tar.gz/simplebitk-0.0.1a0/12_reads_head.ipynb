{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp bam.reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import pysam\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreat the first and last 10 nucleotides of the read\n",
    "# retreat the 10 nucleotides before and after the read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def retreat_upstreams_downstreams_from_read(read,reference_o,sequence_length=10):\n",
    "    '''\n",
    "    '''\n",
    "    read_start = 0\n",
    "    read_end = read.infer_read_length()\n",
    "    mapped_read_start = read.query_alignment_start\n",
    "    mapped_read_end = read.query_alignment_end\n",
    "\n",
    "    read_head = read.seq[0:sequence_length]\n",
    "    read_tail = read.seq[read_end-sequence_length:read_end]\n",
    "    if read_end-sequence_length == read_end:\n",
    "        print(read_end-sequence_length,read_end)\n",
    "    if read_tail == '':\n",
    "        print(read)\n",
    "        print(read_end,read.infer_read_length())\n",
    "        raise ValueError('tes')\n",
    "    mapped_read_head = read.seq[mapped_read_start:mapped_read_start+sequence_length]\n",
    "    mapped_read_tail = read.seq[mapped_read_end-sequence_length:mapped_read_end]\n",
    "    \n",
    "\n",
    "    is_reverse = read.is_reverse\n",
    "    \n",
    "\n",
    "    ref_first = max(0,read.reference_start-sequence_length)\n",
    "    ref_second = read.reference_start if ref_first !=0 else sequence_length\n",
    "    ref_forth = min(reference_o.get_reference_length(read.reference_name),read.reference_end+sequence_length)\n",
    "    ref_third = read.reference_end if ref_forth != reference_o.get_reference_length(read.reference_name) else reference_o.get_reference_length(read.reference_name) - sequence_length\n",
    "\n",
    "    try:\n",
    "        if not is_reverse:\n",
    "            upstream_seq = retreat_seq_from_fasta(reference_o,read.reference_name,ref_first,ref_second,is_reverse)\n",
    "            downstream_seq = retreat_seq_from_fasta(reference_o,read.reference_name,ref_third,ref_forth,is_reverse)\n",
    "        else:\n",
    "            upstream_seq = retreat_seq_from_fasta(reference_o,read.reference_name,ref_third,ref_forth,is_reverse)\n",
    "            downstream_seq = retreat_seq_from_fasta(reference_o,read.reference_name,ref_first,ref_second,is_reverse)\n",
    "    except Exception:\n",
    "        print(read)\n",
    "        raise ValueError('test')\n",
    "\n",
    "    return read_head,read_tail,mapped_read_head,mapped_read_tail,upstream_seq,downstream_seq\n",
    "\n",
    "def retreat_seq_from_fasta(fasta_o,contig,start,end,is_reverse=False):\n",
    "    try:\n",
    "        seq = fasta_o.fetch(contig,start,end)\n",
    "        if is_reverse:\n",
    "            return seq.translate(str.maketrans('ATCG','TAGC'))[::-1]\n",
    "        else:\n",
    "            return seq\n",
    "    except Exception as e:\n",
    "        print(contig,start,end,is_reverse)\n",
    "        raise ValueError(e)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def extract_sequences(bam_file,fasta_file,padding=30,prefix='./extract_sequences'):\n",
    "    '''\n",
    "    extract sequences for motif identification\n",
    "\n",
    "    :param str bam_file: bam file path, with bam.bai in the same directory\n",
    "    :param str fasta_file: reference fasta file path, with fasta.fai in the same directory\n",
    "    :param int padding: the nucleotides number that retreated\n",
    "    :param str prefix: output prefix, there will be six output files\n",
    "    '''\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(prefix)),exist_ok=True)\n",
    "    with pysam.AlignmentFile(bam_file,'r') as bam_o,pysam.FastaFile(fasta_file) as fasta_o,\\\n",
    "        open(f'{prefix}_read_head.fasta','w') as rh,\\\n",
    "            open(f'{prefix}_read_tail.fasta','w') as rt, \\\n",
    "                open(f'{prefix}_mread_head.fasta','w') as mrh, \\\n",
    "                    open(f'{prefix}_mread_tail.fasta','w') as mrt, \\\n",
    "                        open(f'{prefix}_upstream.fasta','w') as us, \\\n",
    "                            open(f'{prefix}_downstream.fasta','w') as ds:\n",
    "        for i, read in enumerate(bam_o):\n",
    "            if i % 10000 == 0:\n",
    "                print(f'reading {i}th reads')\n",
    "            if (not read.is_unmapped) and (not read.is_secondary) and (not read.is_supplementary):\n",
    "                read_head, read_tail, mread_head, mread_tail, upstream, downstream = retreat_upstreams_downstreams_from_read(read,fasta_o,sequence_length=padding)\n",
    "                # if len(read_head) != padding:\n",
    "                #     print(read)\n",
    "                #     print('read head')\n",
    "                # if len(read_tail) != padding:\n",
    "                #     print(read)\n",
    "                #     print('read tail')\n",
    "                # if len(mread_head) != padding:\n",
    "                #     print(read)\n",
    "                #     print('mread head')\n",
    "                # if len(mread_tail) != padding:\n",
    "                #     print(read)\n",
    "                #     print('nread tail')\n",
    "                # if len(downstream) != padding:\n",
    "                #     print(read)\n",
    "                #     print(downstream)\n",
    "                #     print('down')\n",
    "                # if len(upstream) != padding:\n",
    "                #     print(read)\n",
    "                #     print(upstream)\n",
    "                #     print('up')\n",
    "                rh.write(f'>seq_{i}\\n')\n",
    "                rt.write(f'>seq_{i}\\n')\n",
    "                mrh.write(f'>seq_{i}\\n')\n",
    "                mrt.write(f'>seq_{i}\\n')\n",
    "                us.write(f'>seq_{i}\\n')\n",
    "                ds.write(f'>seq_{i}\\n')\n",
    "                rh.write(read_head+'\\n')\n",
    "                rt.write(read_tail+'\\n')\n",
    "                mrh.write(mread_head+'\\n')\n",
    "                mrt.write(mread_tail+'\\n')\n",
    "                us.write(upstream+'\\n')\n",
    "                ds.write(downstream+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading 0th reads\n",
      "reading 10000th reads\n",
      "reading 20000th reads\n",
      "reading 30000th reads\n",
      "reading 40000th reads\n",
      "reading 50000th reads\n",
      "reading 60000th reads\n",
      "reading 70000th reads\n",
      "reading 80000th reads\n",
      "reading 90000th reads\n",
      "reading 100000th reads\n",
      "reading 110000th reads\n",
      "reading 120000th reads\n",
      "reading 130000th reads\n",
      "reading 140000th reads\n",
      "reading 150000th reads\n",
      "reading 160000th reads\n",
      "reading 170000th reads\n",
      "reading 180000th reads\n",
      "reading 190000th reads\n",
      "reading 200000th reads\n",
      "reading 210000th reads\n",
      "reading 220000th reads\n",
      "reading 230000th reads\n",
      "reading 240000th reads\n",
      "reading 250000th reads\n",
      "reading 260000th reads\n",
      "reading 270000th reads\n",
      "reading 280000th reads\n",
      "reading 290000th reads\n",
      "reading 300000th reads\n",
      "reading 310000th reads\n",
      "reading 320000th reads\n",
      "reading 330000th reads\n",
      "reading 340000th reads\n",
      "reading 350000th reads\n",
      "reading 360000th reads\n",
      "reading 370000th reads\n",
      "reading 380000th reads\n",
      "reading 390000th reads\n",
      "reading 400000th reads\n",
      "reading 410000th reads\n",
      "reading 420000th reads\n",
      "reading 430000th reads\n",
      "reading 440000th reads\n",
      "reading 450000th reads\n",
      "reading 460000th reads\n",
      "reading 470000th reads\n",
      "reading 480000th reads\n",
      "reading 490000th reads\n",
      "reading 500000th reads\n",
      "reading 510000th reads\n",
      "reading 520000th reads\n",
      "reading 530000th reads\n",
      "reading 540000th reads\n",
      "reading 550000th reads\n",
      "reading 560000th reads\n",
      "reading 570000th reads\n",
      "reading 580000th reads\n",
      "reading 590000th reads\n",
      "reading 600000th reads\n",
      "reading 610000th reads\n",
      "reading 620000th reads\n",
      "reading 630000th reads\n",
      "reading 640000th reads\n",
      "reading 650000th reads\n",
      "reading 660000th reads\n",
      "reading 670000th reads\n",
      "reading 680000th reads\n",
      "reading 690000th reads\n",
      "reading 700000th reads\n",
      "reading 710000th reads\n",
      "reading 720000th reads\n",
      "reading 730000th reads\n",
      "reading 740000th reads\n",
      "reading 750000th reads\n",
      "reading 760000th reads\n",
      "reading 770000th reads\n",
      "reading 780000th reads\n",
      "reading 790000th reads\n",
      "reading 800000th reads\n",
      "reading 810000th reads\n",
      "reading 820000th reads\n",
      "reading 830000th reads\n",
      "reading 840000th reads\n",
      "reading 850000th reads\n",
      "reading 860000th reads\n",
      "reading 870000th reads\n",
      "reading 880000th reads\n",
      "reading 890000th reads\n",
      "reading 900000th reads\n",
      "reading 910000th reads\n",
      "reading 920000th reads\n",
      "reading 930000th reads\n",
      "reading 940000th reads\n",
      "reading 950000th reads\n",
      "reading 960000th reads\n"
     ]
    }
   ],
   "source": [
    "# bam_file = '/expt/logan/projects/2020-11-09-ONT_DRS_IVT-cellline/results/A549_Cyto_IVT/align/genome_sorted.bam'\n",
    "# fasta_file='/expt/logan/database/homo_sapiens/100/genome.fa'\n",
    "# prefix='tests/test'\n",
    "# extract_sequences(bam_file,fasta_file,padding=30,prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_utils_rd.ipynb.\nConverted 01_pipelines.ipynb.\nConverted 02_mains.ipynb.\nConverted 03_utils_plots.ipynb.\nConverted 04_utils_clusters.ipynb.\nConverted 05_default.ipynb.\nConverted 06_preprocess.ipynb.\nConverted 07_merge_degfc.ipynb.\nConverted 08_polya_sites.ipynb.\nConverted 09_utils_multirun.ipynb.\nConverted 10_utils_bamfilter.ipynb.\nConverted 11_plots.ipynb.\nConverted 12_reads_head.ipynb.\nConverted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
