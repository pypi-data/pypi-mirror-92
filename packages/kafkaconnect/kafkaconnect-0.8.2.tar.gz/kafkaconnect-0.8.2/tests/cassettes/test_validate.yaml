interactions:
- request:
    body: "{\n    \"connect.influx.db\": \"mydb\",\n    \"connect.influx.error.policy\":
      \"THROW\",\n    \"connect.influx.kcql\": \"INSERT INTO t1 SELECT * FROM t1 WITHTIMESTAMP
      sys_time();INSERT INTO t2 SELECT * FROM t2 WITHTIMESTAMP sys_time();INSERT INTO
      t3 SELECT * FROM t3 WITHTIMESTAMP sys_time()\",\n    \"connect.influx.max.retries\":
      \"10\",\n    \"connect.influx.password\": \"\",\n    \"connect.influx.retry.interval\":
      \"60000\",\n    \"connect.influx.url\": \"http://localhost:8086\",\n    \"connect.influx.username\":
      \"-\",\n    \"connect.progress.enabled\": false,\n    \"connector.class\": \"com.datamountaineer.streamreactor.connect.influx.InfluxSinkConnector\",\n
      \   \"name\": \"influxdb-sink\",\n    \"tasks.max\": 1,\n    \"topics\": \"t1,t2,t3\"\n}"
    headers:
      Accept:
      - '*/*'
      Accept-Encoding:
      - gzip, deflate
      Connection:
      - keep-alive
      Content-Length:
      - '697'
      Content-Type:
      - application/json
      User-Agent:
      - python-requests/2.24.0
    method: PUT
    uri: http://localhost:8083/connector-plugins/InfluxSinkConnector/config/validate
  response:
    body:
      string: '{"name":"com.datamountaineer.streamreactor.connect.influx.InfluxSinkConnector","error_count":0,"groups":["Common","Transforms","Error
        Handling","Connection","Miscellaneous","Writes","Metrics"],"configs":[{"definition":{"name":"name","type":"STRING","required":true,"default_value":null,"importance":"HIGH","documentation":"Globally
        unique name to use for this connector.","group":"Common","width":"MEDIUM","display_name":"Connector
        name","dependents":[],"order":1},"value":{"name":"name","value":"influxdb-sink","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"connector.class","type":"STRING","required":true,"default_value":null,"importance":"HIGH","documentation":"Name
        or alias of the class for this connector. Must be a subclass of org.apache.kafka.connect.connector.Connector.
        If the connector is org.apache.kafka.connect.file.FileStreamSinkConnector,
        you can either specify this full name,  or use \"FileStreamSink\" or \"FileStreamSinkConnector\"
        to make the configuration a bit shorter","group":"Common","width":"LONG","display_name":"Connector
        class","dependents":[],"order":2},"value":{"name":"connector.class","value":"com.datamountaineer.streamreactor.connect.influx.InfluxSinkConnector","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"tasks.max","type":"INT","required":false,"default_value":"1","importance":"HIGH","documentation":"Maximum
        number of tasks to use for this connector.","group":"Common","width":"SHORT","display_name":"Tasks
        max","dependents":[],"order":3},"value":{"name":"tasks.max","value":"1","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"key.converter","type":"CLASS","required":false,"default_value":null,"importance":"LOW","documentation":"Converter
        class used to convert between Kafka Connect format and the serialized form
        that is written to Kafka. This controls the format of the keys in messages
        written to or read from Kafka, and since this is independent of connectors
        it allows any connector to work with any serialization format. Examples of
        common formats include JSON and Avro.","group":"Common","width":"SHORT","display_name":"Key
        converter class","dependents":[],"order":4},"value":{"name":"key.converter","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"value.converter","type":"CLASS","required":false,"default_value":null,"importance":"LOW","documentation":"Converter
        class used to convert between Kafka Connect format and the serialized form
        that is written to Kafka. This controls the format of the values in messages
        written to or read from Kafka, and since this is independent of connectors
        it allows any connector to work with any serialization format. Examples of
        common formats include JSON and Avro.","group":"Common","width":"SHORT","display_name":"Value
        converter class","dependents":[],"order":5},"value":{"name":"value.converter","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"header.converter","type":"CLASS","required":false,"default_value":null,"importance":"LOW","documentation":"HeaderConverter
        class used to convert between Kafka Connect format and the serialized form
        that is written to Kafka. This controls the format of the header values in
        messages written to or read from Kafka, and since this is independent of connectors
        it allows any connector to work with any serialization format. Examples of
        common formats include JSON and Avro. By default, the SimpleHeaderConverter
        is used to serialize header values to strings and deserialize them by inferring
        the schemas.","group":"Common","width":"SHORT","display_name":"Header converter
        class","dependents":[],"order":6},"value":{"name":"header.converter","value":null,"recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"transforms","type":"LIST","required":false,"default_value":"","importance":"LOW","documentation":"Aliases
        for the transformations to be applied to records.","group":"Transforms","width":"LONG","display_name":"Transforms","dependents":[],"order":7},"value":{"name":"transforms","value":"","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"config.action.reload","type":"STRING","required":false,"default_value":"restart","importance":"LOW","documentation":"The
        action that Connect should take on the connector when changes in external
        configuration providers result in a change in the connector''s configuration
        properties. A value of ''none'' indicates that Connect will do nothing. A
        value of ''restart'' indicates that Connect should restart/reload the connector
        with the updated configuration properties.The restart may actually be scheduled
        in the future if the external configuration provider indicates that a configuration
        value will expire in the future.","group":"Common","width":"MEDIUM","display_name":"Reload
        Action","dependents":[],"order":8},"value":{"name":"config.action.reload","value":"restart","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"errors.retry.timeout","type":"LONG","required":false,"default_value":"0","importance":"MEDIUM","documentation":"The
        maximum duration in milliseconds that a failed operation will be reattempted.
        The default is 0, which means no retries will be attempted. Use -1 for infinite
        retries.","group":"Error Handling","width":"MEDIUM","display_name":"Retry
        Timeout for Errors","dependents":[],"order":1},"value":{"name":"errors.retry.timeout","value":"0","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"errors.retry.delay.max.ms","type":"LONG","required":false,"default_value":"60000","importance":"MEDIUM","documentation":"The
        maximum duration in milliseconds between consecutive retry attempts. Jitter
        will be added to the delay once this limit is reached to prevent thundering
        herd issues.","group":"Error Handling","width":"MEDIUM","display_name":"Maximum
        Delay Between Retries for Errors","dependents":[],"order":2},"value":{"name":"errors.retry.delay.max.ms","value":"60000","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"errors.tolerance","type":"STRING","required":false,"default_value":"none","importance":"MEDIUM","documentation":"Behavior
        for tolerating errors during connector operation. ''none'' is the default
        value and signals that any error will result in an immediate connector task
        failure; ''all'' changes the behavior to skip over problematic records.","group":"Error
        Handling","width":"SHORT","display_name":"Error Tolerance","dependents":[],"order":3},"value":{"name":"errors.tolerance","value":"none","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"errors.log.enable","type":"BOOLEAN","required":false,"default_value":"false","importance":"MEDIUM","documentation":"If
        true, write each error and the details of the failed operation and problematic
        record to the Connect application log. This is ''false'' by default, so that
        only errors that are not tolerated are reported.","group":"Error Handling","width":"SHORT","display_name":"Log
        Errors","dependents":[],"order":4},"value":{"name":"errors.log.enable","value":"false","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"errors.log.include.messages","type":"BOOLEAN","required":false,"default_value":"false","importance":"MEDIUM","documentation":"Whether
        to the include in the log the Connect record that resulted in a failure. This
        is ''false'' by default, which will prevent record keys, values, and headers
        from being written to log files, although some information such as topic and
        partition number will still be logged.","group":"Error Handling","width":"SHORT","display_name":"Log
        Error Details","dependents":[],"order":5},"value":{"name":"errors.log.include.messages","value":"false","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"topics","type":"LIST","required":false,"default_value":"","importance":"HIGH","documentation":"List
        of topics to consume, separated by commas","group":"Common","width":"LONG","display_name":"Topics","dependents":[],"order":4},"value":{"name":"topics","value":"t1,t2,t3","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"topics.regex","type":"STRING","required":false,"default_value":"","importance":"HIGH","documentation":"Regular
        expression giving topics to consume. Under the hood, the regex is compiled
        to a <code>java.util.regex.Pattern</code>. Only one of topics or topics.regex
        should be specified.","group":"Common","width":"LONG","display_name":"Topics
        regex","dependents":[],"order":4},"value":{"name":"topics.regex","value":"","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"errors.deadletterqueue.topic.name","type":"STRING","required":false,"default_value":"","importance":"MEDIUM","documentation":"The
        name of the topic to be used as the dead letter queue (DLQ) for messages that
        result in an error when processed by this sink connector, or its transformations
        or converters. The topic name is blank by default, which means that no messages
        are to be recorded in the DLQ.","group":"Error Handling","width":"MEDIUM","display_name":"Dead
        Letter Queue Topic Name","dependents":[],"order":6},"value":{"name":"errors.deadletterqueue.topic.name","value":"","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"errors.deadletterqueue.topic.replication.factor","type":"SHORT","required":false,"default_value":"3","importance":"MEDIUM","documentation":"Replication
        factor used to create the dead letter queue topic when it doesn''t already
        exist.","group":"Error Handling","width":"MEDIUM","display_name":"Dead Letter
        Queue Topic Replication Factor","dependents":[],"order":7},"value":{"name":"errors.deadletterqueue.topic.replication.factor","value":"3","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"errors.deadletterqueue.context.headers.enable","type":"BOOLEAN","required":false,"default_value":"false","importance":"MEDIUM","documentation":"If
        true, add headers containing error context to the messages written to the
        dead letter queue. To avoid clashing with headers from the original record,
        all error context header keys, all error context header keys will start with
        <code>__connect.errors.</code>","group":"Error Handling","width":"MEDIUM","display_name":"Enable
        Error Context Headers","dependents":[],"order":8},"value":{"name":"errors.deadletterqueue.context.headers.enable","value":"false","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"connect.influx.url","type":"STRING","required":true,"default_value":null,"importance":"HIGH","documentation":"The
        InfluxDB database url.","group":"Connection","width":"MEDIUM","display_name":"connect.influx.url","dependents":[],"order":1},"value":{"name":"connect.influx.url","value":"http://localhost:8086","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"connect.influx.db","type":"STRING","required":true,"default_value":null,"importance":"HIGH","documentation":"The
        database to store the values to.","group":"Connection","width":"MEDIUM","display_name":"connect.influx.db","dependents":[],"order":2},"value":{"name":"connect.influx.db","value":"mydb","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"connect.influx.username","type":"STRING","required":true,"default_value":null,"importance":"HIGH","documentation":"The
        user to connect to the influx database","group":"Connection","width":"MEDIUM","display_name":"connect.influx.username","dependents":[],"order":3},"value":{"name":"connect.influx.username","value":"-","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"connect.influx.password","type":"PASSWORD","required":false,"default_value":"[hidden]","importance":"HIGH","documentation":"The
        password for the influxdb user.","group":"Connection","width":"MEDIUM","display_name":"connect.influx.password","dependents":[],"order":4},"value":{"name":"connect.influx.password","value":"[hidden]","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"connect.influx.kcql","type":"STRING","required":true,"default_value":null,"importance":"HIGH","documentation":"KCQL
        expression describing field selection and target measurements.","group":"Connection","width":"MEDIUM","display_name":"Kafka
        Connect Query Language","dependents":[],"order":5},"value":{"name":"connect.influx.kcql","value":"INSERT
        INTO t1 SELECT * FROM t1 WITHTIMESTAMP sys_time();INSERT INTO t2 SELECT *
        FROM t2 WITHTIMESTAMP sys_time();INSERT INTO t3 SELECT * FROM t3 WITHTIMESTAMP
        sys_time()","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"connect.influx.error.policy","type":"STRING","required":false,"default_value":"THROW","importance":"HIGH","documentation":"Specifies
        the action to be taken if an error occurs while inserting the data.\nThere
        are two available options:\nNOOP - the error is swallowed\nTHROW - the error
        is allowed to propagate.\nRETRY - The exception causes the Connect framework
        to retry the message. The number of retries is based on\nThe error will be
        logged automatically","group":"Miscellaneous","width":"MEDIUM","display_name":"connect.influx.error.policy","dependents":[],"order":1},"value":{"name":"connect.influx.error.policy","value":"THROW","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"connect.influx.retry.interval","type":"INT","required":false,"default_value":"60000","importance":"MEDIUM","documentation":"The
        time in milliseconds between retries.","group":"Miscellaneous","width":"MEDIUM","display_name":"connect.influx.retry.interval","dependents":[],"order":2},"value":{"name":"connect.influx.retry.interval","value":"60000","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"connect.influx.max.retries","type":"INT","required":false,"default_value":"20","importance":"MEDIUM","documentation":"The
        maximum number of times to try the write again.","group":"Miscellaneous","width":"MEDIUM","display_name":"connect.influx.max.retries","dependents":[],"order":3},"value":{"name":"connect.influx.max.retries","value":"10","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"connect.influx.retention.policy","type":"STRING","required":false,"default_value":"autogen","importance":"HIGH","documentation":"\nDetermines
        how long InfluxDB keeps the data - the options for specifying the duration
        of the retention policy are listed below.\nNote that the minimum retention
        period is one hour.\nDURATION determines how long InfluxDB keeps the data
        - the options for specifying the duration of the retention policy are listed
        below. Note that the minimum retention period is one hour.\nm minutes\nh hours\nd
        days\nw weeks\nINF infinite\n\nDefault retention is `autogen` from 1.0 onwards
        or `default` for any previous version\n    ","group":"Writes","width":"MEDIUM","display_name":"\nDetermines
        how long InfluxDB keeps the data - the options for specifying the duration
        of the retention policy are listed below.\nNote that the minimum retention
        period is one hour.\nDURATION determines how long InfluxDB keeps the data
        - the options for specifying the duration of the retention policy are listed
        below. Note that the minimum retention period is one hour.\nm minutes\nh hours\nd
        days\nw weeks\nINF infinite\n\nDefault retention is `autogen` from 1.0 onwards
        or `default` for any previous version\n    ","dependents":[],"order":1},"value":{"name":"connect.influx.retention.policy","value":"autogen","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"connect.influx.consistency.level","type":"STRING","required":false,"default_value":"ALL","importance":"MEDIUM","documentation":"Specifies
        the write consistency. If any write operations do not meet the configured
        consistency guarantees, an error will occur and the data will not be indexed.
        The default consistency-level is ALL.","group":"Writes","width":"MEDIUM","display_name":"Consistency
        Level","dependents":[],"order":2},"value":{"name":"connect.influx.consistency.level","value":"ALL","recommended_values":[],"errors":[],"visible":true}},{"definition":{"name":"connect.progress.enabled","type":"BOOLEAN","required":false,"default_value":"false","importance":"MEDIUM","documentation":"Enables
        the output for how many records have been processed by the connector","group":"Metrics","width":"MEDIUM","display_name":"Enable
        progress counter","dependents":[],"order":1},"value":{"name":"connect.progress.enabled","value":"false","recommended_values":[],"errors":[],"visible":true}}]}'
    headers:
      Content-Type:
      - application/json
      Date:
      - Thu, 30 Jul 2020 02:39:36 GMT
      Server:
      - Jetty(9.4.18.v20190429)
      Transfer-Encoding:
      - chunked
    status:
      code: 200
      message: OK
version: 1
