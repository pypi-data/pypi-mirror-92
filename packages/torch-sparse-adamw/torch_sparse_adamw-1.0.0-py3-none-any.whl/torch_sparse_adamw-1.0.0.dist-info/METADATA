Metadata-Version: 2.1
Name: torch-sparse-adamw
Version: 1.0.0
Summary: UNKNOWN
Home-page: UNKNOWN
Author: Jonáš Kulhánek
Author-email: jonas.kulhanek@live.com
License: MIT License
Platform: UNKNOWN
Requires-Dist: matplotlib
Requires-Dist: numpy
Requires-Dist: torch
Requires-Dist: gym

# Pytorch Sparse AdamW
This repository contains the sparse version of AdamW optimizer.

The SparseAdamW optimizer behaves like AdamW optimizer, but updates only the statistics for gradients which are computed, in the same way as SparseAdam optimizer. The optimizer can only be used on modules, which produce sparse gradients, e.g., nn.Embedding.

## Install
Install by running:
```bash
pip install torch-sparse-adamw
```


